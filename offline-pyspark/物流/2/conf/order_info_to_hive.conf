# 自动生成的SeaTunnel配置文件
# 源表: tms.order_info
# 目标表: ods_order_info
# 生成时间: 2025-07-21 21:23:30
# 提示: Hive DDL已单独生成到ddl_scripts目录

env {
  execution.parallelism = 1
  job.mode = "BATCH"
}

source {
  Jdbc {
    url = "jdbc:mysql://cdh03:3306/tms"
    driver = "com.mysql.jdbc.Driver"
    connection_check_timeout_sec = 100
    user = "root"
    password = "root"
    table_path = "tms.order_info"
    query = "SELECT `id`, `order_no`, `status`, `collect_type`, `user_id`, `receiver_complex_id`, `receiver_province_id`, `receiver_city_id`, `receiver_district_id`, `receiver_address`, `receiver_name`, `receiver_phone`, `receive_location`, `sender_complex_id`, `sender_province_id`, `sender_city_id`, `sender_district_id`, `sender_address`, `sender_name`, `sender_phone`, `send_location`, `payment_type`, `cargo_num`, `amount`, `estimate_arrive_time`, `distance`, `create_time`, `update_time`, `is_deleted`,date_format(create_time,'%Y%m%d') as ds FROM `tms`.`order_info`"

  }
}

transform {
 
}

sink {
  Hive {
    table_name = "tms.ods_order_info"
    metastore_uri = "thrift://cdh01:9083"
    hive.hadoop.conf-path = "/etc/hadoop/conf"
    save_mode = "overwrite"
    partition_by = ["ds"]
    dynamic_partition = true
    file_format = "orc"
    orc_compress = "SNAPPY"
    tbl_properties = {
            "external.table.purge" = "true"
        }
    fields = ['id', 'order_no', 'status', 'collect_type', 'user_id', 'receiver_complex_id', 'receiver_province_id', 'receiver_city_id', 'receiver_district_id', 'receiver_address', 'receiver_name', 'receiver_phone', 'receive_location', 'sender_complex_id', 'sender_province_id', 'sender_city_id', 'sender_district_id', 'sender_address', 'sender_name', 'sender_phone', 'send_location', 'payment_type', 'cargo_num', 'amount', 'estimate_arrive_time', 'distance', 'create_time', 'update_time', 'is_deleted']
  }
}
