from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, round, when, lit
import os

# é…ç½® Spark è¿è¡Œç¯å¢ƒï¼ˆå…³è” Anaconda ç¯å¢ƒï¼‰
os.environ["PYSPARK_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"
os.environ["PYSPARK_DRIVER_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"


def init_spark():
    """åˆå§‹åŒ– SparkSessionï¼Œé…ç½®æœ¬åœ°æ¨¡å¼"""
    return SparkSession.builder \
       .appName("Product360Dashboard") \
       .config("spark.driver.memory", "4g") \
       .config("spark.executor.memory", "4g") \
       .master("local[*]") \
       .getOrCreate()


def load_data(spark, data_path):
    """è¯»å–åŸå§‹ CSV æ•°æ®ï¼Œè‡ªåŠ¨æ¨æ–­ Schema"""
    return spark.read \
       .format("csv") \
       .option("header", "true") \
       .option("inferSchema", "true") \
       .load(data_path)


def calculate_ads_bu(df):
    """
    è®¡ç®—ä¸åˆ†å±‚æŒ‡æ ‡ï¼ˆads_bu è¡¨ï¼‰ï¼š
    æŒ‰ dtã€platformã€channel èšåˆï¼Œè¦†ç›–ç”¨æˆ·è§„æ¨¡ã€è¥æ”¶ã€ç•™å­˜ã€è¡Œä¸ºæŒ‡æ ‡
    """
    return df.groupBy("date", "platform", "channel") \
       .agg(
            # 1. ç”¨æˆ·è§„æ¨¡æŒ‡æ ‡
            sum("new_users").alias("new_users"),
            sum("active_users").alias("active_users"),
            sum("paying_users").alias("paying_users"),

            # 2. è¥æ”¶è½¬åŒ–æŒ‡æ ‡
            sum("total_revenue").alias("total_revenue"),
            round(sum("total_revenue") / sum("active_users"), 2).alias("arpu"),
            round(sum("total_revenue") / sum("paying_users"), 2).alias("arppu"),
            round(sum("paying_users") / sum("active_users"), 4).alias("conversion_rate"),

            # 3. ç•™å­˜æŒ‡æ ‡
            avg("retention_rate_1d").alias("retention_rate_1d"),
            avg("retention_rate_7d").alias("retention_rate_7d"),
            avg("retention_rate_30d").alias("retention_rate_30d"),

            # 4. ç”¨æˆ·è¡Œä¸ºæŒ‡æ ‡
            sum("session_count").alias("session_count"),
            avg("average_session_duration").alias("avg_session_duration"),
            sum("page_views").alias("page_views"),
            round(sum("page_views") / sum("session_count"), 2).alias("avg_page_views"),
            avg("bounce_rate").alias("bounce_rate")
        ) \
       .withColumnRenamed("date", "dt")  # ç»Ÿä¸€æ—¥æœŸå­—æ®µä¸º dt


def calculate_ads_rfm(df):
    """
    è®¡ç®— RFM åˆ†å±‚æŒ‡æ ‡ï¼ˆads_rfm è¡¨ï¼‰ï¼š
    1. å…ˆæŒ‰ rfm_total åˆ†å±‚ï¼ˆé«˜/ä¸­/ä½ä»·å€¼ï¼‰
    2. å†æŒ‰ dtã€platformã€channelã€rfm_layer èšåˆ
    """
    # åˆ†å±‚è§„åˆ™ï¼šrfm_total â‰¥12=é«˜ä»·å€¼ï¼Œâ‰¥8=ä¸­ç­‰ï¼Œå¦åˆ™ä½ä»·å€¼
    df_with_layer = df.withColumn(
        "rfm_layer",
        when(col("rfm_total") >= 12, "é«˜ä»·å€¼ç”¨æˆ·")
       .when(col("rfm_total") >= 8, "ä¸­ç­‰ä»·å€¼ç”¨æˆ·")
       .otherwise("ä½ä»·å€¼ç”¨æˆ·")
    )

    return df_with_layer.groupBy("date", "platform", "channel", "rfm_layer") \
       .agg(
            # 1. ç”¨æˆ·è§„æ¨¡æŒ‡æ ‡ï¼ˆä¸ ads_bu ä¸€è‡´ï¼‰
            sum("new_users").alias("new_users"),
            sum("active_users").alias("active_users"),
            sum("paying_users").alias("paying_users"),

            # 2. è¥æ”¶è½¬åŒ–æŒ‡æ ‡ï¼ˆä¸ ads_bu ä¸€è‡´ï¼‰
            sum("total_revenue").alias("total_revenue"),
            round(sum("total_revenue") / sum("active_users"), 2).alias("arpu"),
            round(sum("total_revenue") / sum("paying_users"), 2).alias("arppu"),
            round(sum("paying_users") / sum("active_users"), 4).alias("conversion_rate"),

            # 3. ç•™å­˜æŒ‡æ ‡ï¼ˆä¸ ads_bu ä¸€è‡´ï¼‰
            avg("retention_rate_1d").alias("retention_rate_1d"),
            avg("retention_rate_7d").alias("retention_rate_7d"),

            # 4. ç”¨æˆ·è¡Œä¸ºæŒ‡æ ‡ï¼ˆä¸ ads_bu ä¸€è‡´ï¼‰
            sum("session_count").alias("session_count"),
            avg("bounce_rate").alias("bounce_rate")
        ) \
       .withColumnRenamed("date", "dt")  # ç»Ÿä¸€æ—¥æœŸå­—æ®µä¸º dt


def save_csv(df, output_dir, table_name):
    """
    ä¿å­˜ä¸º CSV æ–‡ä»¶ï¼š
    1. åˆå¹¶åˆ†åŒºä¸º 1 ä¸ªæ–‡ä»¶ï¼ˆcoalesce(1)ï¼‰
    2. è¦†ç›–æ—§æ–‡ä»¶ï¼Œç¡®ä¿ç›®å½•å¹²å‡€
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # å®Œæ•´è·¯å¾„
    full_path = os.path.join(output_dir, f"{table_name}.csv")

    # åˆ é™¤æ—§æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(full_path):
        os.remove(full_path)

    # å†™å…¥ CSVï¼ˆåˆå¹¶åˆ†åŒºï¼‰
    df.coalesce(1) \
        .write \
        .format("csv") \
        .option("header", "true") \
        .mode("overwrite") \
        .save(output_dir)

    # Spark ä¼šç”Ÿæˆ part-00000 æ–‡ä»¶ï¼Œæ‰‹åŠ¨é‡å‘½åä¸ºç›®æ ‡æ–‡ä»¶å
    part_file = [f for f in os.listdir(output_dir) if f.startswith("part-")][0]
    os.rename(os.path.join(output_dir, part_file), full_path)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚ .crc æ–‡ä»¶ï¼‰
    for f in os.listdir(output_dir):
        if f.endswith(".crc"):
            os.remove(os.path.join(output_dir, f))

    print(f"âœ… è¡¨ [{table_name}] å·²ä¿å­˜è‡³ï¼š{full_path}")


def main():
    # é…ç½®è·¯å¾„
    data_path = "D:/Pycharm1/day1/å·¥å•/03-å•†å“ä¸»é¢˜-å•†å“360çœ‹æ¿/1.ç”Ÿæˆæ•°æ®/metrics_data_with_rfm/mock_with_rfm.csv"
    output_dir = "ads_bu_360"  # è¾“å‡ºç›®å½•
    output_dir1 = "ads_rfm_360"  # è¾“å‡ºç›®å½•

    # åˆå§‹åŒ– Spark
    spark = init_spark()

    # è¯»å–åŸå§‹æ•°æ®
    raw_df = load_data(spark, data_path)
    print(f"ğŸ”¹ åŸå§‹æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {raw_df.count()} è¡Œ")

    # 1. è®¡ç®—ä¸åˆ†å±‚æŒ‡æ ‡ï¼ˆads_buï¼‰
    df_ads_bu = calculate_ads_bu(raw_df)
    save_csv(df_ads_bu, output_dir, "ads_bu")

    # 2. è®¡ç®— RFM åˆ†å±‚æŒ‡æ ‡ï¼ˆads_rfmï¼‰
    df_ads_rfm = calculate_ads_rfm(raw_df)
    save_csv(df_ads_rfm, output_dir1, "ads_rfm")

    # é¢„è§ˆç»“æœï¼ˆç¤ºä¾‹ï¼‰
    print("\nğŸ“Š ä¸åˆ†å±‚æŒ‡æ ‡ç¤ºä¾‹ï¼ˆads_buï¼‰ï¼š")
    df_ads_bu.select("dt", "platform", "channel", "new_users", "total_revenue").show(3)

    print("\nğŸ“Š RFM åˆ†å±‚æŒ‡æ ‡ç¤ºä¾‹ï¼ˆads_rfmï¼‰ï¼š")
    df_ads_rfm.select("dt", "platform", "channel", "rfm_layer", "active_users").show(3)

    # åœæ­¢ Spark
    spark.stop()


if __name__ == "__main__":
    main()