from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, TimestampType, BooleanType
from pyspark.sql.functions import col, to_timestamp, when, current_timestamp, expr
import os
import pandas as pd
from datetime import datetime, timedelta
import random

# å·¥å•ç¼–å·ï¼šå¤§æ•°æ® - ç”µå•†æ•°ä»“ - 08 - å•†å“ä¸»é¢˜è¥é”€å·¥å…·å®¢æœä¸“å±ä¼˜æƒ çœ‹æ¿

# é…ç½®PySparkç¯å¢ƒ
os.environ["PYSPARK_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"
os.environ["PYSPARK_DRIVER_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"

# åˆå§‹åŒ–SparkSession
spark = SparkSession.builder \
    .appName("ç”ŸæˆDWDæ˜ç»†æ•°æ®å±‚è¡¨") \
    .master("local[*]") \
    .getOrCreate()

# åˆ›å»ºå­˜å‚¨ç›®å½•
output_root = "mock_data/dwd"
ods_root = "mock_data"  # ODSå±‚è·¯å¾„
dim_root = "mock_data"  # DIMå±‚è·¯å¾„

# åˆ›å»ºæ‰€æœ‰éœ€è¦çš„ç›®å½•
for dir_path in [output_root, ods_root, dim_root]:
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)


# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
def generate_mock_data():
    """ç”Ÿæˆå¿…è¦çš„æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
    # ç”Ÿæˆåº—é“ºç»´åº¦è¡¨
    if not os.path.exists(f"{dim_root}/dim_shop.csv"):
        data = {
            "shop_id": ["shop_001", "shop_002", "shop_003"],
            "shop_name": ["æ——èˆ°åº—", "ä¸“è¥åº—", "æŠ˜æ‰£åº—"],
            "platform": ["æ·˜å®", "äº¬ä¸œ", "æ‹¼å¤šå¤š"],
            "etl_update_time": [datetime.now() for _ in range(3)]
        }
        pd.DataFrame(data).to_csv(f"{dim_root}/dim_shop.csv", index=False)

    # ç”Ÿæˆå•†å“ç»´åº¦è¡¨
    if not os.path.exists(f"{dim_root}/dim_product.csv"):
        data = {
            "product_id": ["prod_001", "prod_002", "prod_003", "prod_004"],
            "product_name": ["æ‰‹æœº", "ç”µè„‘", "å¹³æ¿", "è€³æœº"],
            "price": [5999.99, 7999.99, 3999.99, 1299.99],
            "shop_id": ["shop_001", "shop_001", "shop_002", "shop_003"],
            "status": ["åœ¨å”®", "åœ¨å”®", "åœ¨å”®", "ä¸‹æ¶"],
            "etl_update_time": [datetime.now() for _ in range(4)]
        }
        pd.DataFrame(data).to_csv(f"{dim_root}/dim_product.csv", index=False)

    # ç”ŸæˆSKUç»´åº¦è¡¨
    if not os.path.exists(f"{dim_root}/dim_sku.csv"):
        data = {
            "sku_id": ["sku_001", "sku_002", "sku_003", "sku_004"],
            "product_id": ["prod_001", "prod_001", "prod_002", "prod_003"],
            "product_name": ["æ‰‹æœº", "æ‰‹æœº", "ç”µè„‘", "å¹³æ¿"],
            "spec": ["128G é»‘è‰²", "256G ç™½è‰²", "512G ç°è‰²", "256G é“¶è‰²"],
            "sku_price": [5999.99, 6499.99, 8999.99, 4299.99],
            "etl_update_time": [datetime.now() for _ in range(4)]
        }
        pd.DataFrame(data).to_csv(f"{dim_root}/dim_sku.csv", index=False)

    # ç”Ÿæˆå®¢æœç»´åº¦è¡¨
    if not os.path.exists(f"{dim_root}/dim_customer_service.csv"):
        data = {
            "customer_service_id": ["cs_001", "cs_002", "cs_003"],
            "cs_name": ["å¼ ä¸‰", "æå››", "ç‹äº”"],
            "shop_id": ["shop_001", "shop_002", "shop_001"],
            "etl_update_time": [datetime.now() for _ in range(3)]
        }
        pd.DataFrame(data).to_csv(f"{dim_root}/dim_customer_service.csv", index=False)

    # ç”Ÿæˆæ´»åŠ¨è¡¨
    if not os.path.exists(f"{ods_root}/ods_customer_service_promo_activity.csv"):
        data = {
            "activity_id": ["act_001", "act_002", "act_003"],
            "activity_name": ["618å¤§ä¿ƒ", "åº—åº†æ´»åŠ¨", "æ–°å“æ¨å¹¿"],
            "activity_level": ["å•†å“çº§", "SKUçº§", "å•†å“çº§"],
            "promo_type": ["å›ºå®šä¼˜æƒ ", "è‡ªå®šä¹‰ä¼˜æƒ ", "å›ºå®šä¼˜æƒ "],
            "custom_promo_max": [100, 200, None],
            "start_time": [(datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d %H:%M:%S"),
                           (datetime.now() - timedelta(days=3)).strftime("%Y-%m-%d %H:%M:%S"),
                           datetime.now().strftime("%Y-%m-%d %H:%M:%S")],
            "end_time": [(datetime.now() + timedelta(days=7)).strftime("%Y-%m-%d %H:%M:%S"),
                         (datetime.now() + timedelta(days=10)).strftime("%Y-%m-%d %H:%M:%S"),
                         (datetime.now() + timedelta(days=15)).strftime("%Y-%m-%d %H:%M:%S")],
            "status": ["è¿›è¡Œä¸­", "è¿›è¡Œä¸­", "æœªå¼€å§‹"],
            "shop_id": ["shop_001", "shop_002", "shop_001"],
            "create_time": [(datetime.now() - timedelta(days=10)).strftime("%Y-%m-%d %H:%M:%S"),
                            (datetime.now() - timedelta(days=5)).strftime("%Y-%m-%d %H:%M:%S"),
                            (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S")],
            "raw_extra": ["", "", ""]
        }
        pd.DataFrame(data).to_csv(f"{ods_root}/ods_customer_service_promo_activity.csv", index=False)

    # ç”Ÿæˆæ´»åŠ¨-å•†å“å…³è”è¡¨
    if not os.path.exists(f"{ods_root}/ods_customer_service_promo_activity_item.csv"):
        data = {
            "id": ["item_001", "item_002", "item_003", "item_004"],
            "activity_id": ["act_001", "act_001", "act_002", "act_003"],
            "product_id": ["prod_001", "prod_002", "prod_003", "prod_001"],
            "sku_id": ["sku_001", None, "sku_003", "sku_002"],
            "promo_amount": [500, 800, 300, 600],
            "limit_purchase_count": [5, 3, 10, 2],
            "is_removed": ["å¦", "å¦", "å¦", "æ˜¯"],
            "create_time": [(datetime.now() - timedelta(days=9)).strftime("%Y-%m-%d %H:%M:%S"),
                            (datetime.now() - timedelta(days=9)).strftime("%Y-%m-%d %H:%M:%S"),
                            (datetime.now() - timedelta(days=4)).strftime("%Y-%m-%d %H:%M:%S"),
                            (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S")]
        }
        pd.DataFrame(data).to_csv(f"{ods_root}/ods_customer_service_promo_activity_item.csv", index=False)

    # ç”Ÿæˆä¼˜æƒ å‘é€è¡¨
    if not os.path.exists(f"{ods_root}/ods_customer_service_promo_send.csv"):
        data = {
            "send_id": ["send_001", "send_002", "send_003", "send_004"],
            "activity_id": ["act_001", "act_001", "act_002", "act_001"],
            "product_id": ["prod_001", "prod_002", "prod_003", "prod_001"],
            "sku_id": ["sku_001", None, "sku_003", "sku_002"],
            "customer_service_id": ["cs_001", "cs_001", "cs_002", "cs_003"],
            "consumer_id": ["user_001", "user_002", "user_003", "user_004"],
            "actual_promo_amount": [500, 800, 300, 600],
            "valid_duration": [24, 12, 8, 4],
            "send_time": [(datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d %H:%M:%S"),
                          (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S"),
                          (datetime.now() - timedelta(hours=12)).strftime("%Y-%m-%d %H:%M:%S"),
                          (datetime.now() - timedelta(hours=2)).strftime("%Y-%m-%d %H:%M:%S")],
            "remark": ["æ–°ç”¨æˆ·ä¼˜æƒ ", "", "è€å®¢æˆ·å›é¦ˆ", "é™æ—¶ä¼˜æƒ "]
        }
        pd.DataFrame(data).to_csv(f"{ods_root}/ods_customer_service_promo_send.csv", index=False)

    # ç”Ÿæˆä¼˜æƒ æ ¸é”€è¡¨
    if not os.path.exists(f"{ods_root}/ods_customer_service_promo_use.csv"):
        data = {
            "use_id": ["use_001", "use_002", "use_003"],
            "send_id": ["send_001", "send_002", "send_003"],
            "order_id": ["order_001", "order_002", "order_003"],
            "pay_time": [(datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S"),
                         (datetime.now() - timedelta(hours=10)).strftime("%Y-%m-%d %H:%M:%S"),
                         (datetime.now() - timedelta(hours=2)).strftime("%Y-%m-%d %H:%M:%S")],
            "pay_amount": [5499.99, 7199.99, 3699.99],
            "purchase_count": [1, 1, 2]
        }
        pd.DataFrame(data).to_csv(f"{ods_root}/ods_customer_service_promo_use.csv", index=False)


# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
generate_mock_data()

# ------------------------------
# 1. æ´»åŠ¨æ˜ç»†äº‹å®è¡¨ï¼šdwd_customer_service_promo_activity
# æ¥æºï¼šods_customer_service_promo_activity + dim_shop
# ------------------------------
# è¯»å–ODSå±‚æ´»åŠ¨è¡¨
ods_activity = spark.read \
    .option("header", "true") \
    .schema("""
        activity_id STRING, activity_name STRING, activity_level STRING, promo_type STRING,
        custom_promo_max INT, start_time STRING, end_time STRING, status STRING,
        shop_id STRING, create_time STRING, raw_extra STRING
    """) \
    .csv(f"{ods_root}/ods_customer_service_promo_activity.csv")

# è¯»å–DIMå±‚åº—é“ºè¡¨
dim_shop = spark.read \
    .option("header", "true") \
    .schema("shop_id STRING, shop_name STRING, platform STRING, etl_update_time TIMESTAMP") \
    .csv(f"{dim_root}/dim_shop.csv")

# æ•°æ®æ¸…æ´—ä¸è½¬æ¢
dwd_activity = ods_activity \
    .withColumn("activity_level",
                when(col("activity_level") == "å•†å“çº§", "product_level")
                .when(col("activity_level") == "SKUçº§", "sku_level")
                .otherwise("unknown")) \
    .withColumn("promo_type",
                when(col("promo_type") == "å›ºå®šä¼˜æƒ ", "fixed")
                .when(col("promo_type") == "è‡ªå®šä¹‰ä¼˜æƒ ", "custom")
                .otherwise("unknown")) \
    .withColumn("start_time", to_timestamp(col("start_time"), "yyyy-MM-dd HH:mm:ss")) \
    .withColumn("end_time", to_timestamp(col("end_time"), "yyyy-MM-dd HH:mm:ss")) \
    .withColumn("create_time", to_timestamp(col("create_time"), "yyyy-MM-dd HH:mm:ss")) \
    .withColumn("status",
                when(col("status") == "æœªå¼€å§‹", "not_started")
                .when(col("status") == "è¿›è¡Œä¸­", "ongoing")
                .when(col("status") == "å·²ç»“æŸ", "ended")
                .otherwise("unknown")) \
    .join(dim_shop.select("shop_id", "shop_name"), on="shop_id", how="left") \
    .withColumn("etl_load_time", current_timestamp()) \
    .select("activity_id", "activity_name", "activity_level", "promo_type",
            "custom_promo_max", "start_time", "end_time", "status",
            "shop_id", "shop_name", "create_time", "etl_load_time")

# å†™å…¥DWDå±‚
dwd_activity.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dwd_customer_service_promo_activity")

# ------------------------------
# 2. æ´»åŠ¨-å•†å“å…³è”æ˜ç»†äº‹å®è¡¨ï¼šdwd_customer_service_promo_activity_item
# æ¥æºï¼šods_customer_service_promo_activity_item + dim_product + dim_sku
# ------------------------------
# è¯»å–ODSå±‚æ´»åŠ¨-å•†å“å…³è”è¡¨
ods_item = spark.read \
    .option("header", "true") \
    .schema("""
        id STRING, activity_id STRING, product_id STRING, sku_id STRING,
        promo_amount INT, limit_purchase_count INT, is_removed STRING, create_time STRING
    """) \
    .csv(f"{ods_root}/ods_customer_service_promo_activity_item.csv")

# è¯»å–DIMå±‚å•†å“è¡¨å’ŒSKUè¡¨
dim_product = spark.read \
    .option("header", "true") \
    .schema(
    "product_id STRING, product_name STRING, price DECIMAL(10,2), shop_id STRING, status STRING, etl_update_time TIMESTAMP") \
    .csv(f"{dim_root}/dim_product.csv")

dim_sku = spark.read \
    .option("header", "true") \
    .schema(
    "sku_id STRING, product_id STRING, product_name STRING, spec STRING, sku_price DECIMAL(10,2), etl_update_time TIMESTAMP") \
    .csv(f"{dim_root}/dim_sku.csv")

# æ•°æ®æ¸…æ´—ä¸è½¬æ¢
dwd_item = ods_item \
    .withColumn("is_removed", when(col("is_removed") == "æ˜¯", True).otherwise(False)) \
    .withColumn("create_time", to_timestamp(col("create_time"), "yyyy-MM-dd HH:mm:ss")) \
    .withColumn("promo_amount",
                when(col("promo_amount") > 5000, 5000)  # è¶…å‡ºä¸Šé™æˆªæ–­
                .otherwise(col("promo_amount"))) \
    .join(dim_product.select("product_id", "product_name"), on="product_id", how="left") \
    .join(dim_sku.select("sku_id", "spec").withColumnRenamed("spec", "sku_spec"),
          on="sku_id", how="left") \
    .withColumn("etl_load_time", current_timestamp()) \
    .select("id", "activity_id", "product_id", "product_name",
            "sku_id", "sku_spec", "promo_amount", "limit_purchase_count",
            "is_removed", "create_time", "etl_load_time")

# å†™å…¥DWDå±‚
dwd_item.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dwd_customer_service_promo_activity_item")

# ------------------------------
# 3. ä¼˜æƒ å‘é€æ˜ç»†äº‹å®è¡¨ï¼šdwd_customer_service_promo_send
# æ¥æºï¼šods_customer_service_promo_send + dim_customer_service + dim_product
# ------------------------------
# è¯»å–ODSå±‚ä¼˜æƒ å‘é€è¡¨
ods_send = spark.read \
    .option("header", "true") \
    .schema("""
        send_id STRING, activity_id STRING, product_id STRING, sku_id STRING,
        customer_service_id STRING, consumer_id STRING, actual_promo_amount INT,
        valid_duration INT, send_time STRING, remark STRING
    """) \
    .csv(f"{ods_root}/ods_customer_service_promo_send.csv")

# è¯»å–DIMå±‚å®¢æœè¡¨
dim_cs = spark.read \
    .option("header", "true") \
    .schema("customer_service_id STRING, cs_name STRING, shop_id STRING, etl_update_time TIMESTAMP") \
    .csv(f"{dim_root}/dim_customer_service.csv")

# æ•°æ®æ¸…æ´—ä¸è½¬æ¢
dwd_send = ods_send \
    .withColumn("send_time", to_timestamp(col("send_time"), "yyyy-MM-dd HH:mm:ss")) \
    .withColumn("expire_time", expr("send_time + make_interval(0, 0, 0, 0, valid_duration, 0, 0)")) \
    .withColumn("valid_duration",
                when(col("valid_duration") < 1, 1)
                .when(col("valid_duration") > 24, 24)
                .otherwise(col("valid_duration"))) \
    .withColumn("is_expired", current_timestamp() > col("expire_time")) \
    .join(dim_cs.select("customer_service_id", "cs_name"), on="customer_service_id", how="left") \
    .join(dim_product.select("product_id", "product_name"), on="product_id", how="left") \
    .withColumn("etl_load_time", current_timestamp()) \
    .select("send_id", "activity_id", "product_id", "product_name",
            "sku_id", "customer_service_id", "cs_name", "consumer_id",
            "actual_promo_amount", "valid_duration", "send_time", "expire_time",
            "remark", "is_expired", "etl_load_time")

# å†™å…¥DWDå±‚
dwd_send.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dwd_customer_service_promo_send")

# ------------------------------
# 4. ä¼˜æƒ æ ¸é”€æ˜ç»†äº‹å®è¡¨ï¼šdwd_customer_service_promo_use
# æ¥æºï¼šods_customer_service_promo_use + dwd_customer_service_promo_send
# ------------------------------
# è¯»å–ODSå±‚ä¼˜æƒ æ ¸é”€è¡¨
ods_use = spark.read \
    .option("header", "true") \
    .schema("""
        use_id STRING, send_id STRING, order_id STRING, pay_time STRING,
        pay_amount DECIMAL(10,2), purchase_count INT
    """) \
    .csv(f"{ods_root}/ods_customer_service_promo_use.csv")

# æ•°æ®æ¸…æ´—ä¸è½¬æ¢
dwd_use = ods_use \
    .withColumn("pay_time", to_timestamp(col("pay_time"), "yyyy-MM-dd HH:mm:ss")) \
    .join(dwd_send.select("send_id", "activity_id", "product_id", "consumer_id", "expire_time"),
          on="send_id", how="left") \
    .withColumn("is_valid_use", col("pay_time") <= col("expire_time")) \
    .withColumn("etl_load_time", current_timestamp()) \
    .select("use_id", "send_id", "activity_id", "order_id",
            "product_id", "consumer_id", "pay_time", "pay_amount",
            "purchase_count", "is_valid_use", "etl_load_time")

# å†™å…¥DWDå±‚
dwd_use.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dwd_customer_service_promo_use")

print(f"DWDæ˜ç»†æ•°æ®å±‚è¡¨å·²ç”Ÿæˆè‡³ï¼š{os.path.abspath(output_root)}ï¼Œå…±4å¼ è¡¨")
spark.stop()



# ä½ æä¾›çš„ DWD å±‚ä»£ç ç¬¦åˆã€Šå¤§æ•°æ® - ç”µå•†æ•°ä»“ - 08 - å•†å“ä¸»é¢˜è¥é”€å·¥å…·çœ‹æ¿ V1.2-20250125.pdfã€‹ä¸­å®¢æœä¸“å±ä¼˜æƒ ä¸šåŠ¡çš„æ˜ç»†æ•°æ®å¤„ç†éœ€æ±‚ï¼Œä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
#
# ä¸šåŠ¡è§„åˆ™ä¸¥æ ¼é€‚é…
# æ´»åŠ¨çº§åˆ«æ ‡å‡†åŒ–ä¸º â€œproduct_levelâ€â€œsku_levelâ€ï¼Œå¯¹åº”æ–‡æ¡£ä¸­ â€œå•†å“çº§ & SKU çº§ä¼˜æƒ â€ çš„åˆ’åˆ†ğŸ”¶3-77ğŸ”¶3-79ï¼›ä¼˜æƒ ç±»å‹è½¬æ¢ä¸º â€œfixedâ€â€œcustomâ€ï¼ŒåŒ¹é… â€œå›ºå®šä¼˜æƒ  & è‡ªå®šä¹‰ä¼˜æƒ â€ çš„ä¸šåŠ¡é…ç½®ğŸ”¶3-86ğŸ”¶3-88ã€‚
# ä¼˜æƒ é‡‘é¢æ ¡éªŒé€»è¾‘ï¼ˆè¶…å‡º 5000 å…ƒæˆªæ–­ï¼‰ç¬¦åˆæ–‡æ¡£ä¸­ â€œä¼˜æƒ é‡‘é¢ä¸è¶…è¿‡ 5000 å…ƒâ€ çš„çº¦æŸğŸ”¶3-99ğŸ”¶3-100ï¼›æœ‰æ•ˆæœŸé™åˆ¶åœ¨ 1-24 å°æ—¶ï¼Œä¸ â€œæ¶ˆè´¹è€…å¯ä½¿ç”¨æ•ˆæœŸç”±å®¢æœè‡ªå®šä¹‰å¡«å†™ï¼ˆ24 å°æ—¶ä»¥å†…ï¼‰â€ çš„è§„åˆ™ä¸€è‡´ğŸ”¶3-81ğŸ”¶3-206ã€‚
# æ ¸é”€æœ‰æ•ˆæ€§åˆ¤æ–­ï¼ˆis_valid_useï¼‰åŸºäº â€œæ”¯ä»˜æ—¶é—´â‰¤è¿‡æœŸæ—¶é—´â€ï¼Œè§£å†³äº†æ–‡æ¡£ä¸­ â€œä¼˜æƒ ä½¿ç”¨å‘¨æœŸ 24 å°æ—¶å¯¼è‡´æ”¯ä»˜æ¬¡æ•°å¯èƒ½å¤§äºå‘é€æ¬¡æ•°â€ çš„è·¨å¤©åœºæ™¯ğŸ”¶3-41ã€‚
# æ•°æ®æ¸…æ´—é€»è¾‘è´´åˆä¸šåŠ¡åœºæ™¯
# æ—¶é—´å­—æ®µç»Ÿä¸€è½¬æ¢ä¸ºTimestampTypeï¼Œç¡®ä¿æ´»åŠ¨èµ·æ­¢æ—¶é—´ã€å‘é€æ—¶é—´ã€æ”¯ä»˜æ—¶é—´ç­‰çš„æ—¶é—´ç»´åº¦åˆ†æä¸€è‡´æ€§ã€‚
# å…³è” DIM å±‚è¡¥å……å•†å“åç§°ã€åº—é“ºåç§°ã€å®¢æœå§“åç­‰æè¿°æ€§ä¿¡æ¯ï¼Œä½¿æ˜ç»†æ•°æ®æ—¢èƒ½æ”¯æ’‘æŒ‡æ ‡è®¡ç®—ï¼Œåˆèƒ½æ»¡è¶³ä¸šåŠ¡äººå‘˜å¯¹ â€œè°ã€åœ¨ä»€ä¹ˆæ´»åŠ¨ä¸­ã€å‘é€äº†ä»€ä¹ˆå•†å“ä¼˜æƒ â€ çš„è¿½æº¯éœ€æ±‚ğŸ”¶3-36ğŸ”¶3-37ã€‚
# æ´»åŠ¨çŠ¶æ€æ ‡å‡†åŒ–ä¸º â€œnot_startedâ€â€œongoingâ€â€œendedâ€ï¼Œä¸æ–‡æ¡£ä¸­æ´»åŠ¨ç®¡ç†çš„ â€œæœªå¼€å§‹ / è¿›è¡Œä¸­ / å·²ç»“æŸâ€ çŠ¶æ€æµè½¬é€»è¾‘å¯¹åº”ğŸ”¶3-122ğŸ”¶3-130ã€‚
# è¡¨é—´å…³è”å®Œæ•´è¦†ç›–ä¸šåŠ¡é“¾è·¯
# é€šè¿‡activity_idå…³è”æ´»åŠ¨ä¸å•†å“ï¼Œsend_idå…³è”å‘é€ä¸æ ¸é”€è®°å½•ï¼Œå®Œæ•´ä¿ç•™ä» â€œæ´»åŠ¨åˆ›å»ºâ†’å•†å“å…³è”â†’ä¼˜æƒ å‘é€â†’æ¶ˆè´¹è€…æ ¸é”€â€ çš„å…¨é“¾è·¯æ•°æ®ç—•è¿¹ï¼Œä¸ºåç»­ DWS å±‚æŒ‰æ´»åŠ¨ã€å®¢æœã€å•†å“ç­‰ç»´åº¦æ±‡æ€»æŒ‡æ ‡æä¾›äº†ç»†ç²’åº¦çš„äº‹å®ä¾æ®ğŸ”¶3-29ğŸ”¶3-34ã€‚
#
# ä»£ç ç”Ÿæˆçš„ 4 å¼  DWD è¡¨å¯ç›´æ¥æ”¯æ’‘å®¢æœä¸“å±ä¼˜æƒ æ´»åŠ¨æ•ˆæœçš„æ˜ç»†åˆ†æï¼Œå¦‚å•æ¡ä¼˜æƒ å‘é€çš„æ ¸é”€æƒ…å†µã€æ´»åŠ¨å…³è”å•†å“çš„ä¼˜æƒ åŠ›åº¦åˆ†å¸ƒç­‰ï¼Œä¸ºä¸Šå±‚æ±‡æ€»æä¾›äº†å¯é çš„æ˜ç»†æ•°æ®åŸºç¡€ã€‚
