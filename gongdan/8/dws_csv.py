import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, TimestampType, DateType
from pyspark.sql.functions import col, count, sum, round, current_timestamp, to_date, when, lit, datediff

# å·¥å•ç¼–å·ï¼šå¤§æ•°æ® - ç”µå•†æ•°ä»“ - 08 - å•†å“ä¸»é¢˜è¥é”€å·¥å…·å®¢æœä¸“å±ä¼˜æƒ çœ‹æ¿


# é…ç½®PySparkç¯å¢ƒ
os.environ["PYSPARK_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"
os.environ["PYSPARK_DRIVER_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"

# åˆå§‹åŒ–SparkSession
spark = SparkSession.builder \
    .appName("ç”ŸæˆDWSæ±‡æ€»æ•°æ®å±‚è¡¨") \
    .master("local[*]") \
    .getOrCreate()

# åˆ›å»ºå­˜å‚¨ç›®å½•
output_root = "mock_data/dws"
dwd_root = "mock_data"
dim_root = "mock_data"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
for dir_path in [output_root, dwd_root, dim_root]:
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)


# ------------------------------
# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å¹¶ä¿å­˜ä¸ºCSVæ–‡ä»¶
# ------------------------------

# ç”Ÿæˆæ¨¡æ‹Ÿçš„å®¢æœç»´åº¦æ•°æ®
def generate_dim_customer_service():
    data = {
        "customer_service_id": [f"cs_{i:03d}" for i in range(1, 11)],
        "cs_name": [f"å®¢æœ{i}" for i in range(1, 11)],
        "shop_id": [f"shop_{i % 3 + 1}" for i in range(1, 11)]
    }
    df = pd.DataFrame(data)
    df.to_csv(f"{dim_root}/dim_customer_service.csv", index=False)
    return df


# ç”Ÿæˆæ¨¡æ‹Ÿçš„åº—é“ºç»´åº¦æ•°æ®
def generate_dim_shop():
    data = {
        "shop_id": ["shop_1", "shop_2", "shop_3"],
        "shop_name": ["æ——èˆ°åº—", "ä¸“è¥åº—", "ä¸“å–åº—"],
        "platform": ["æ·˜å®", "äº¬ä¸œ", "æ‹¼å¤šå¤š"]
    }
    df = pd.DataFrame(data)
    df.to_csv(f"{dim_root}/dim_shop.csv", index=False)
    return df


# ç”Ÿæˆæ¨¡æ‹Ÿçš„æ´»åŠ¨æ•°æ®
def generate_dwd_activity():
    data = {
        "activity_id": [f"act_{i:03d}" for i in range(1, 6)],
        "activity_name": [f"æ´»åŠ¨{i}" for i in range(1, 6)],
        "activity_level": ["A", "B", "A", "C", "B"],
        "promo_type": ["æ»¡å‡", "æŠ˜æ‰£", "æ»¡å‡", "èµ å“", "æŠ˜æ‰£"],
        "status": ["è¿›è¡Œä¸­", "å·²ç»“æŸ", "è¿›è¡Œä¸­", "æœªå¼€å§‹", "è¿›è¡Œä¸­"]
    }
    df = pd.DataFrame(data)
    df.to_csv(f"{dwd_root}/dwd_customer_service_promo_activity.csv", index=False)
    return df


# ç”Ÿæˆæ¨¡æ‹Ÿçš„å‘é€æ•°æ®
def generate_dwd_send():
    np.random.seed(42)
    dates = [(datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d %H:%M:%S") for i in range(30)]

    data = {
        "send_id": [f"send_{i:05d}" for i in range(1, 1001)],
        "customer_service_id": [f"cs_{np.random.randint(1, 11):03d}" for _ in range(1000)],
        "activity_id": [f"act_{np.random.randint(1, 6):03d}" for _ in range(1000)],
        "product_id": [f"prod_{np.random.randint(1, 21)}" for _ in range(1000)],
        "product_name": [f"å•†å“{np.random.randint(1, 21)}" for _ in range(1000)],
        "sku_id": [f"sku_{np.random.randint(1, 51)}" if np.random.random() > 0.3 else None for _ in range(1000)],
        "sku_spec": [f"{np.random.choice(['çº¢è‰²', 'è“è‰²', 'é»‘è‰²'])}-{np.random.choice(['S', 'M', 'L', 'XL'])}"
                     if np.random.random() > 0.3 else None for _ in range(1000)],
        "send_time": [np.random.choice(dates) for _ in range(1000)]
    }
    df = pd.DataFrame(data)
    df.to_csv(f"{dwd_root}/dwd_customer_service_promo_send.csv", index=False)
    return df


# ç”Ÿæˆæ¨¡æ‹Ÿçš„æ ¸é”€æ•°æ®
def generate_dwd_use(send_df):
    np.random.seed(43)
    # éšæœºé€‰æ‹©30%çš„å‘é€è®°å½•è¿›è¡Œæ ¸é”€
    used_send_ids = np.random.choice(send_df["send_id"], size=int(len(send_df) * 0.3), replace=False)

    dates = [(datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d %H:%M:%S") for i in range(15)]

    data = {
        "use_id": [f"use_{i:05d}" for i in range(1, len(used_send_ids) + 1)],
        "send_id": used_send_ids,
        "pay_time": [np.random.choice(dates) for _ in range(len(used_send_ids))],
        "is_valid_use": [np.random.choice(["true", "false"], p=[0.9, 0.1]) for _ in range(len(used_send_ids))]
    }
    df = pd.DataFrame(data)
    df.to_csv(f"{dwd_root}/dwd_customer_service_promo_use.csv", index=False)
    return df


# ç”Ÿæˆæ‰€æœ‰æ¨¡æ‹Ÿæ•°æ®
dim_cs_df = generate_dim_customer_service()
dim_shop_df = generate_dim_shop()
activity_df = generate_dwd_activity()
send_df = generate_dwd_send()
use_df = generate_dwd_use(send_df)

# ------------------------------
# 1. æ¯æ—¥æ±‡æ€»è¡¨ï¼šdws_customer_service_promo_daily
# ç»´åº¦ï¼šæ—¥æœŸ+åº—é“º | æŒ‡æ ‡ï¼šå‘é€/æ ¸é”€æ ¸å¿ƒæ•°æ®
# ------------------------------
dws_daily_schema = StructType([
    StructField("stat_date", DateType(), False),
    StructField("shop_id", StringType(), False),
    StructField("shop_name", StringType(), False),
    StructField("platform", StringType(), False),
    StructField("total_send_count", IntegerType(), False),
    StructField("total_use_count", IntegerType(), False),
    StructField("valid_use_count", IntegerType(), False),
    StructField("use_rate", DecimalType(5, 2), False),
    StructField("etl_update_time", TimestampType(), False)
])

# è¯»å–DWDå‘é€è¡¨ï¼ˆä½¿ç”¨æ­£ç¡®è·¯å¾„ï¼‰
dwd_send = spark.read \
    .option("header", "true") \
    .csv(f"{dwd_root}/dwd_customer_service_promo_send.csv")

# è¯»å–DWDæ ¸é”€è¡¨
dwd_use = spark.read \
    .option("header", "true") \
    .csv(f"{dwd_root}/dwd_customer_service_promo_use.csv")

# è¯»å–åº—é“ºç»´åº¦è¡¨
dim_shop = spark.read \
    .option("header", "true") \
    .csv(f"{dim_root}/dim_shop.csv")

# è¯»å–å®¢æœç»´åº¦è¡¨ï¼ˆç”¨äºè·å–shop_idï¼‰
dim_cs = spark.read \
    .option("header", "true") \
    .csv(f"{dim_root}/dim_customer_service.csv")

# å…³é”®ä¿®å¤ï¼šdwd_sendå…³è”dim_csè·å–shop_id
dwd_send_with_shop = dwd_send.join(
    dim_cs.select("customer_service_id", "shop_id"),
    on="customer_service_id",
    how="left"
)

# æŒ‰æ—¥æœŸ+åº—é“ºèšåˆï¼ˆä½¿ç”¨å¸¦shop_idçš„å‘é€è¡¨ï¼‰
daily_agg = dwd_send_with_shop \
    .withColumn("stat_date", to_date(col("send_time"))) \
    .groupBy("stat_date", "shop_id") \
    .agg(count("send_id").alias("total_send_count")) \
    .join(
    # æ ¸é”€è¡¨ä¹Ÿéœ€è¦å…³è”å®¢æœè¡¨è·å–shop_id
    dwd_use.join(dwd_send.select("send_id", "customer_service_id"), on="send_id", how="left")
    .join(dim_cs.select("customer_service_id", "shop_id"), on="customer_service_id", how="left")
    .withColumn("stat_date", to_date(col("pay_time")))
    .groupBy("stat_date", "shop_id")
    .agg(
        count("use_id").alias("total_use_count"),
        sum(when(col("is_valid_use") == "true", 1).otherwise(0)).alias("valid_use_count")
    ),
    on=["stat_date", "shop_id"],
    how="full_outer"
) \
    .join(dim_shop.select("shop_id", "shop_name", "platform"), on="shop_id", how="left") \
    .fillna(0, subset=["total_send_count", "total_use_count", "valid_use_count"]) \
    .withColumn("use_rate",
                when(col("total_send_count") == 0, 0)
                .otherwise(round(col("total_use_count") / col("total_send_count") * 100, 2))
                ) \
    .withColumn("etl_update_time", current_timestamp())

dws_daily = daily_agg.select(
    "stat_date", "shop_id", "shop_name", "platform",
    "total_send_count", "total_use_count", "valid_use_count",
    "use_rate", "etl_update_time"
)

dws_daily.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dws_customer_service_promo_daily")

# ------------------------------
# 2. æ´»åŠ¨æ±‡æ€»è¡¨ï¼šdws_customer_service_promo_activity
# ç»´åº¦ï¼šæ´»åŠ¨ | æŒ‡æ ‡ï¼šæ´»åŠ¨æ•ˆæœæ•°æ®
# ------------------------------
dws_activity_schema = StructType([
    StructField("activity_id", StringType(), False),
    StructField("activity_name", StringType(), False),
    StructField("activity_level", StringType(), False),
    StructField("promo_type", StringType(), False),
    StructField("activity_status", StringType(), False),
    StructField("total_send_count", IntegerType(), False),
    StructField("total_use_count", IntegerType(), False),
    StructField("7d_send_count", IntegerType(), False),
    StructField("7d_use_count", IntegerType(), False),
    StructField("use_rate", DecimalType(5, 2), False),
    StructField("etl_update_time", TimestampType(), False)
])

# è¯»å–DWDæ´»åŠ¨è¡¨
dwd_activity = spark.read \
    .option("header", "true") \
    .csv(f"{dwd_root}/dwd_customer_service_promo_activity.csv")

# æ´»åŠ¨å‘é€æ•°æ®èšåˆï¼ˆä½¿ç”¨å¸¦shop_idçš„å‘é€è¡¨ï¼‰
activity_send = dwd_send_with_shop \
    .withColumn("is_7d", datediff(current_timestamp(), to_date(col("send_time"))) <= 7) \
    .groupBy("activity_id") \
    .agg(
    count("send_id").alias("total_send_count"),
    sum(when(col("is_7d"), 1).otherwise(0)).alias("7d_send_count")
)

# æ´»åŠ¨æ ¸é”€æ•°æ®èšåˆ - ä¿®å¤ï¼šæ˜ç¡®æŒ‡å®šactivity_idæ¥æºï¼Œé¿å…é‡å¤
activity_use = dwd_use \
    .join(
    dwd_send_with_shop.select("send_id", "activity_id", "send_time"),
    on="send_id",
    how="left"
) \
    .select(
    col("send_id"),
    col("activity_id"),  # æ˜ç¡®ä½¿ç”¨å‘é€è¡¨çš„activity_id
    col("pay_time"),
    col("use_id"),
    col("send_time")
) \
    .withColumn("is_7d", datediff(current_timestamp(), to_date(col("pay_time"))) <= 7) \
    .groupBy("activity_id") \
    .agg(
    count("use_id").alias("total_use_count"),
    sum(when(col("is_7d"), 1).otherwise(0)).alias("7d_use_count")
)

# å…³è”æ´»åŠ¨ç»´åº¦ä¿¡æ¯
dws_activity = dwd_activity \
    .join(activity_send, on="activity_id", how="left") \
    .join(activity_use, on="activity_id", how="left") \
    .fillna(0, subset=["total_send_count", "total_use_count", "7d_send_count", "7d_use_count"]) \
    .withColumn("use_rate",
                when(col("total_send_count") == 0, 0)
                .otherwise(round(col("total_use_count") / col("total_send_count") * 100, 2))
                ) \
    .withColumnRenamed("status", "activity_status") \
    .withColumn("etl_update_time", current_timestamp()) \
    .select(
    "activity_id", "activity_name", "activity_level", "promo_type",
    "activity_status", "total_send_count", "total_use_count",
    "7d_send_count", "7d_use_count", "use_rate", "etl_update_time"
)

dws_activity.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dws_customer_service_promo_activity")

# ------------------------------
# 3. å®¢æœæ±‡æ€»è¡¨ï¼šdws_customer_service_promo_cs
# ç»´åº¦ï¼šå®¢æœ+å‘¨æœŸ | æŒ‡æ ‡ï¼šå®¢æœç»©æ•ˆæ•°æ®
# ------------------------------
dws_cs_schema = StructType([
    StructField("customer_service_id", StringType(), False),
    StructField("cs_name", StringType(), False),
    StructField("shop_id", StringType(), False),
    StructField("stat_period", StringType(), False),
    StructField("send_count", IntegerType(), False),
    StructField("use_count", IntegerType(), False),
    StructField("use_rate", DecimalType(5, 2), False),
    StructField("etl_update_time", TimestampType(), False)
])

# å¤šå‘¨æœŸèšåˆï¼ˆæ—¥/7å¤©/30å¤©ï¼‰
periods = [("day", 1), ("7days", 7), ("30days", 30)]
cs_dfs = []
for period_name, days in periods:
    cs_agg = dwd_send_with_shop \
        .withColumn("in_period", datediff(current_timestamp(), to_date(col("send_time"))) <= days) \
        .groupBy("customer_service_id") \
        .agg(sum(when(col("in_period"), 1).otherwise(0)).alias("send_count")) \
        .join(
        dwd_use.join(dwd_send_with_shop.select("send_id", "customer_service_id", "send_time"), on="send_id", how="left")
        .withColumn("in_period", datediff(current_timestamp(), to_date(col("pay_time"))) <= days)
        .groupBy("customer_service_id")
        .agg(sum(when(col("in_period"), 1).otherwise(0)).alias("use_count")),
        on="customer_service_id",
        how="left"
    ) \
        .join(dim_cs, on="customer_service_id", how="left") \
        .fillna(0, subset=["send_count", "use_count"]) \
        .withColumn("use_rate",
                    when(col("send_count") == 0, 0)
                    .otherwise(round(col("use_count") / col("send_count") * 100, 2))
                    ) \
        .withColumn("stat_period", lit(period_name)) \
        .withColumn("etl_update_time", current_timestamp()) \
        .select(
            "customer_service_id",
            "cs_name",
            "shop_id",
            "stat_period",
            "send_count",
            "use_count",
            "use_rate",
            "etl_update_time"
        )

    cs_dfs.append(cs_agg)

# åˆå¹¶å¤šå‘¨æœŸæ•°æ®ï¼ˆæ­¤æ—¶å­—æ®µé¡ºåºå·²ä¸schemaä¸€è‡´ï¼Œæ— éœ€å†æŒ‡å®šschemaï¼‰
dws_cs = spark.createDataFrame(
    spark.sparkContext.union([df.rdd for df in cs_dfs])
).select(
    "customer_service_id", "cs_name", "shop_id", "stat_period",
    "send_count", "use_count", "use_rate", "etl_update_time"
)

dws_cs.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dws_customer_service_promo_cs")


# ------------------------------
# 4. å•†å“æ±‡æ€»è¡¨ï¼šdws_customer_service_promo_product
# ç»´åº¦ï¼šå•†å“/SKU | æŒ‡æ ‡ï¼šå•†å“ä¼˜æƒ æ•ˆæœ
# ------------------------------
dws_product_schema = StructType([
    StructField("product_id", StringType(), False),
    StructField("product_name", StringType(), False),
    StructField("sku_id", StringType(), True),
    StructField("sku_spec", StringType(), True),
    StructField("total_send_count", IntegerType(), False),
    StructField("total_use_count", IntegerType(), False),
    StructField("use_rate", DecimalType(5, 2), False),
    StructField("etl_update_time", TimestampType(), False)
])

# å•†å“/skuç»´åº¦èšåˆ
product_agg = dwd_send_with_shop \
    .groupBy("product_id", "product_name", "sku_id", "sku_spec") \
    .agg(count("send_id").alias("total_send_count")) \
    .join(
    dwd_use.join(dwd_send_with_shop.select("send_id", "product_id", "product_name", "sku_id", "sku_spec"), on="send_id",
                 how="left")
    .groupBy("product_id", "product_name", "sku_id", "sku_spec")
    .agg(count("use_id").alias("total_use_count")),
    on=["product_id", "product_name", "sku_id", "sku_spec"],
    how="full_outer"
) \
    .fillna(0, subset=["total_send_count", "total_use_count"]) \
    .withColumn("use_rate",
                when(col("total_send_count") == 0, 0)
                .otherwise(round(col("total_use_count") / col("total_send_count") * 100, 2))
                ) \
    .withColumn("etl_update_time", current_timestamp())

dws_product = product_agg.select(
    "product_id", "product_name", "sku_id", "sku_spec",
    "total_send_count", "total_use_count", "use_rate", "etl_update_time"
)

dws_product.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dws_customer_service_promo_product")

print(f"DWSæ±‡æ€»æ•°æ®å±‚è¡¨å·²ç”Ÿæˆè‡³ï¼š{os.path.abspath(output_root)}ï¼Œå…±4å¼ è¡¨")
spark.stop()



# ä½ æä¾›çš„ DWS å±‚ä»£ç ç¬¦åˆã€Šå¤§æ•°æ® - ç”µå•†æ•°ä»“ - 08 - å•†å“ä¸»é¢˜è¥é”€å·¥å…·çœ‹æ¿ V1.2-20250125.pdfã€‹ä¸­å®¢æœä¸“å±ä¼˜æƒ ä¸šåŠ¡çš„æ±‡æ€»éœ€æ±‚ï¼Œä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
#
# ä¸šåŠ¡è§„åˆ™é€‚é…
# æ—¶é—´ç»´åº¦æ”¯æŒ â€œæ—¥ / 7 å¤© / 30 å¤©â€ï¼Œä¸æ–‡æ¡£ä¸­çœ‹æ¿çš„æ—¶é—´ç­›é€‰è¦æ±‚ä¸€è‡´ï¼Œå¯æ»¡è¶³ä¸åŒå‘¨æœŸçš„æ•°æ®åˆ†æéœ€æ±‚ğŸ”¶2-30ã€‚
# æ´»åŠ¨æ±‡æ€»è¡¨åŒºåˆ† â€œè¿›è¡Œä¸­ / å·²ç»“æŸâ€ ç­‰çŠ¶æ€ï¼Œå¯¹åº”æ–‡æ¡£ä¸­ â€œæ´»åŠ¨æ•ˆæœå±•ç¤ºä¸åŒçŠ¶æ€ä¸‹å•ä¸ªæ´»åŠ¨æ•ˆæœæ•°æ®â€ çš„è¦æ±‚ğŸ”¶2-29ã€‚
# æ ¸é”€ç‡è®¡ç®—é€»è¾‘è¦†ç›–è·¨å¤©åœºæ™¯ï¼Œç¬¦åˆ â€œä¼˜æƒ ä½¿ç”¨å‘¨æœŸ 24 å°æ—¶å¯èƒ½å¯¼è‡´æ”¯ä»˜æ¬¡æ•°å¤§äºå‘é€æ¬¡æ•°â€ çš„ä¸šåŠ¡ç‰¹ç‚¹ğŸ”¶2-41ã€‚
# æ•°æ®æµè½¬åˆç†æ€§
# é€šè¿‡å…³è” DWD å±‚æ˜ç»†æ•°æ®ï¼ˆå‘é€è¡¨ã€æ ¸é”€è¡¨ï¼‰å’Œ DIM å±‚ç»´åº¦æ•°æ®ï¼ˆåº—é“ºã€å®¢æœï¼‰ï¼Œå®ç°äº†ä»æ˜ç»†åˆ°æ±‡æ€»çš„åˆ†å±‚åŠ å·¥ï¼Œç¬¦åˆ â€œODSâ†’DIMâ†’DWDâ†’DWSâ€ çš„æ•°ä»“æµè½¬é€»è¾‘ã€‚
# å…³é”®ä¿®å¤ï¼ˆå¦‚dwd_sendå…³è”dim_csè·å–shop_idï¼‰è§£å†³äº†è¡¨é—´å…³è”æ–­å±‚é—®é¢˜ï¼Œç¡®ä¿åº—é“ºç»´åº¦çš„æ±‡æ€»æŒ‡æ ‡å‡†ç¡®ã€‚
# æŒ‡æ ‡è®¾è®¡è´´åˆçœ‹æ¿éœ€æ±‚
# åŒ…å« â€œå‘é€æ¬¡æ•°â€â€œæ ¸é”€æ¬¡æ•°â€â€œæ ¸é”€ç‡â€ ç­‰æ ¸å¿ƒæŒ‡æ ‡ï¼Œç›´æ¥å¯¹åº”æ–‡æ¡£ä¸­ â€œå·¥å…·æ•ˆæœæ€»è§ˆâ€â€œæ´»åŠ¨æ•ˆæœâ€â€œå®¢æœæ•°æ®â€ ç­‰æ¨¡å—çš„å±•ç¤ºå†…å®¹ğŸ”¶2-32ğŸ”¶2-34ğŸ”¶2-36ã€‚
# å•†å“æ±‡æ€»è¡¨æŒ‰ â€œå•†å“ / SKUâ€ ç»´åº¦èšåˆï¼Œé€‚é…æ–‡æ¡£ä¸­ â€œå•†å“çº§ / SKU çº§ä¼˜æƒ â€ çš„ä¸šåŠ¡åˆ’åˆ†ï¼Œå¯æ”¯æ’‘ä¸åŒç²’åº¦çš„å•†å“ä¼˜æƒ æ•ˆæœåˆ†æğŸ”¶2-77ğŸ”¶2-79ã€‚
#
# ä»£ç ç”Ÿæˆçš„ 4 å¼  DWS è¡¨ä»¥ CSV æ ¼å¼å­˜å‚¨äºmock_data/dwsç›®å½•ï¼Œä¸ä¸Šå±‚è¡¨å­˜å‚¨é£æ ¼ä¸€è‡´ï¼Œä¸ºåç»­ ADS å±‚çš„çœ‹æ¿å±•ç¤ºæä¾›äº†é¢„èšåˆçš„æŒ‡æ ‡æ•°æ®ã€‚

