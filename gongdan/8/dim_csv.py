from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DecimalType, TimestampType
from pyspark.sql.functions import col, to_timestamp, regexp_replace, current_timestamp, when, substring, concat, lit
import os
from datetime import datetime

# å·¥å•ç¼–å·ï¼šå¤§æ•°æ® - ç”µå•†æ•°ä»“ - 08 - å•†å“ä¸»é¢˜è¥é”€å·¥å…·å®¢æœä¸“å±ä¼˜æƒ çœ‹æ¿

# é…ç½®PySparkç¯å¢ƒ
os.environ["PYSPARK_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"
os.environ["PYSPARK_DRIVER_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"

# åˆå§‹åŒ–SparkSession
spark = SparkSession.builder \
    .appName("ç”ŸæˆDIMç»´åº¦è¡¨å±‚è¡¨") \
    .master("local[*]") \
    .getOrCreate()

# åˆ›å»ºå­˜å‚¨ç›®å½•
output_root = "mock_data/dim"
if not os.path.exists(output_root):
    os.makedirs(output_root)
ods_root = "mock_data"  # ODSå±‚æ•°æ®è·¯å¾„ï¼ˆç›´æ¥æŒ‡å‘mock_dataç›®å½•ï¼‰

# ------------------------------
# 1. å•†å“ç»´åº¦è¡¨ï¼šdim_product
# æ¥æºï¼šods_product.csv
# ------------------------------
product_schema = StructType([
    StructField("product_id", StringType(), False),
    StructField("product_name", StringType(), False),
    StructField("price", DecimalType(10, 2), False),
    StructField("shop_id", StringType(), False),
    StructField("status", StringType(), False),  # æ ‡å‡†åŒ–ä¸ºï¼šonsale/offsale
    StructField("etl_update_time", TimestampType(), False)
])

# è¯»å–ODSå±‚å•†å“è¡¨ï¼ˆä¿®æ­£è·¯å¾„ï¼Œç›´æ¥è¯»å–CSVæ–‡ä»¶ï¼‰
ods_product = spark.read \
    .option("header", "true") \
    .schema("product_id STRING, product_name STRING, price DECIMAL(10,2), shop_id STRING, status STRING") \
    .csv(f"{ods_root}/ods_product.csv")  # è¯»å–mock_dataä¸‹çš„ods_product.csv

# ä¿®å¤çŠ¶æ€è½¬æ¢é€»è¾‘
dim_product = ods_product \
    .withColumn("status",
                when(col("status") == "åœ¨å”®", "onsale")
                .when(col("status") == "ä¸‹æ¶", "offsale")
                .otherwise("unknown")) \
    .withColumn("etl_update_time", current_timestamp()) \
    .select("product_id", "product_name", "price", "shop_id", "status", "etl_update_time")

# å†™å…¥DIMå±‚
dim_product.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dim_product")

# ------------------------------
# 2. SKUç»´åº¦è¡¨ï¼šdim_sku
# æ¥æºï¼šods_sku.csv + dim_productï¼ˆå…³è”å•†å“ä¿¡æ¯ï¼‰
# ------------------------------
sku_schema = StructType([
    StructField("sku_id", StringType(), False),
    StructField("product_id", StringType(), False),
    StructField("product_name", StringType(), False),  # å…³è”å•†å“åç§°
    StructField("spec", StringType(), False),
    StructField("sku_price", DecimalType(10, 2), False),
    StructField("etl_update_time", TimestampType(), False)
])

# è¯»å–ODSå±‚SKUè¡¨ï¼ˆä¿®æ­£è·¯å¾„ï¼‰
ods_sku = spark.read \
    .option("header", "true") \
    .schema("sku_id STRING, product_id STRING, spec STRING, sku_price DECIMAL(10,2)") \
    .csv(f"{ods_root}/ods_sku.csv")  # è¯»å–mock_dataä¸‹çš„ods_sku.csv

# å…³è”å•†å“ç»´åº¦è¡¨è¡¥å……å•†å“åç§°
dim_sku = ods_sku.join(
    dim_product.select("product_id", "product_name"),
    on="product_id",
    how="left"
).withColumn("etl_update_time", current_timestamp()) \
    .select("sku_id", "product_id", "product_name", "spec", "sku_price", "etl_update_time")

# å†™å…¥DIMå±‚
dim_sku.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dim_sku")

# ------------------------------
# 3. å®¢æœç»´åº¦è¡¨ï¼šdim_customer_service
# æ¥æºï¼šods_customer_service.csv
# ------------------------------
cs_schema = StructType([
    StructField("customer_service_id", StringType(), False),
    StructField("cs_name", StringType(), False),
    StructField("shop_id", StringType(), False),
    StructField("etl_update_time", TimestampType(), False)
])

# è¯»å–ODSå±‚å®¢æœè¡¨ï¼ˆä¿®æ­£è·¯å¾„ï¼‰
ods_cs = spark.read \
    .option("header", "true") \
    .schema("customer_service_id STRING, cs_name STRING, shop_id STRING") \
    .csv(f"{ods_root}/ods_customer_service.csv")  # è¯»å–mock_dataä¸‹çš„ods_customer_service.csv

dim_cs = ods_cs \
    .withColumn("cs_name", regexp_replace(col("cs_name"), "[^a-zA-Z0-9\u4e00-\u9fa5]", "")) \
    .withColumn("etl_update_time", current_timestamp()) \
    .select("customer_service_id", "cs_name", "shop_id", "etl_update_time")

# å†™å…¥DIMå±‚
dim_cs.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dim_customer_service")

# ------------------------------
# 4. åº—é“ºç»´åº¦è¡¨ï¼šdim_shop
# æ¥æºï¼šODSå±‚å„è¡¨æå–
# ------------------------------
shop_schema = StructType([
    StructField("shop_id", StringType(), False),
    StructField("shop_name", StringType(), False),  # æ¨¡æ‹Ÿåº—é“ºåç§°
    StructField("platform", StringType(), False),  # æ·˜å®/å¤©çŒ«
    StructField("etl_update_time", TimestampType(), False)
])

# ä»å•†å“è¡¨æå–å”¯ä¸€åº—é“ºIDå¹¶è¡¥å……ä¿¡æ¯
shop_data = ods_product.select("shop_id").distinct() \
    .withColumn("shop_name", concat(substring(col("shop_id"), 5, 10), lit("åº—é“º"))) \
    .withColumn("platform",
                when(col("shop_id").like("%tmall%"), "tmall")
                .when(col("shop_id").like("%taobao%"), "taobao")
                .otherwise("unknown")) \
    .withColumn("etl_update_time", current_timestamp())

dim_shop = spark.createDataFrame(shop_data.rdd, schema=shop_schema)

# å†™å…¥DIMå±‚
dim_shop.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dim_shop")

# ------------------------------
# 5. æ´»åŠ¨ç±»å‹ç»´åº¦è¡¨ï¼šdim_activity_type
# ------------------------------
activity_type_schema = StructType([
    StructField("type_code", StringType(), False),
    StructField("type_name", StringType(), False),
    StructField("type_category", StringType(), False),  # æ´»åŠ¨çº§åˆ«/ä¼˜æƒ ç±»å‹
    StructField("description", StringType(), True),
    StructField("etl_update_time", TimestampType(), False)
])

# è·å–å½“å‰æ—¶é—´æˆ³ä½œä¸ºå›ºå®šå€¼ï¼ˆè€Œä¸æ˜¯Columnå¯¹è±¡ï¼‰
current_time = datetime.now()

# æ‰‹åŠ¨å½•å…¥ç»´åº¦æ•°æ®ï¼ˆä½¿ç”¨å®é™…æ—¶é—´æˆ³å€¼è€ŒéColumnå¯¹è±¡ï¼‰
activity_type_data = [
    ("product_level", "å•†å“çº§", "activity_level", "åŒä¸€å•†å“ä¸‹æ‰€æœ‰SKUåŒä»·", current_time),
    ("sku_level", "SKUçº§", "activity_level", "åŒä¸€å•†å“ä¸‹ä¸åŒSKUå¯è®¾ä¸åŒä»·", current_time),
    ("fixed", "å›ºå®šä¼˜æƒ ", "promo_type", "ä¼˜æƒ é‡‘é¢å›ºå®š", current_time),
    ("custom", "è‡ªå®šä¹‰ä¼˜æƒ ", "promo_type", "å®¢æœå¯åœ¨ä¸Šé™å†…è‡ªç”±å¡«å†™é‡‘é¢", current_time)
]

dim_activity_type = spark.createDataFrame(activity_type_data, schema=activity_type_schema)

# å†™å…¥DIMå±‚
dim_activity_type.write.mode("overwrite").option("header", "true").csv(f"{output_root}/dim_activity_type")

print(f"DIMç»´åº¦è¡¨å±‚è¡¨å·²ç”Ÿæˆè‡³ï¼š{os.path.abspath(output_root)}ï¼Œå…±5å¼ è¡¨")
spark.stop()


# ä½ æä¾›çš„ DIM å±‚ä»£ç ç¬¦åˆã€Šå¤§æ•°æ® - ç”µå•†æ•°ä»“ - 08 - å•†å“ä¸»é¢˜è¥é”€å·¥å…·çœ‹æ¿ V1.2-20250125.pdfã€‹ä¸­å¯¹å®¢æœä¸“å±ä¼˜æƒ ä¸šåŠ¡çš„ç»´åº¦æ•°æ®è¦æ±‚ï¼Œä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
#
# ç»´åº¦è¡¨è®¾è®¡é€‚é…ä¸šåŠ¡è§„åˆ™
# å•†å“ç»´åº¦è¡¨ï¼ˆdim_productï¼‰å¯¹å•†å“çŠ¶æ€è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆâ€œåœ¨å”®â€â†’â€œonsaleâ€ã€â€œä¸‹æ¶â€â†’â€œoffsaleâ€ï¼‰ï¼Œä¸æ–‡æ¡£ä¸­å•†å“å‚ä¸ä¼˜æƒ æ´»åŠ¨çš„çŠ¶æ€ç®¡ç†é€»è¾‘ä¸€è‡´ğŸ”¶3-92ã€‚
# SKU ç»´åº¦è¡¨ï¼ˆdim_skuï¼‰å…³è”å•†å“åç§°ï¼Œæ¸…æ™°åŒºåˆ† â€œå•†å“çº§â€ ä¸ â€œSKU çº§â€ ä¼˜æƒ çš„é€‚ç”¨èŒƒå›´ï¼Œç¬¦åˆæ–‡æ¡£ä¸­æ´»åŠ¨çº§åˆ«çš„åˆ’åˆ†è§„åˆ™ğŸ”¶3-77ğŸ”¶3-79ã€‚
# æ´»åŠ¨ç±»å‹ç»´åº¦è¡¨ï¼ˆdim_activity_typeï¼‰æ˜ç¡® â€œå›ºå®šä¼˜æƒ â€â€œè‡ªå®šä¹‰ä¼˜æƒ â€ çš„ç¼–ç ä¸æè¿°ï¼Œå¯¹åº”æ–‡æ¡£ä¸­ä¼˜æƒ ç±»å‹çš„æ ¸å¿ƒé…ç½®é¡¹ğŸ”¶3-86ğŸ”¶3-88ã€‚
# æ•°æ®æ¸…æ´—é€»è¾‘è´´åˆä¸šåŠ¡éœ€æ±‚
# å®¢æœç»´åº¦è¡¨ï¼ˆdim_customer_serviceï¼‰å»é™¤å§“åä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œç¡®ä¿å®¢æœä¿¡æ¯è§„èŒƒï¼Œé€‚é…åç»­å®¢æœæ•°æ®ç»Ÿè®¡åœºæ™¯ğŸ”¶3-36ã€‚
# åº—é“ºç»´åº¦è¡¨ï¼ˆdim_shopï¼‰åŒºåˆ† â€œæ·˜å®â€â€œå¤©çŒ«â€ å¹³å°ï¼Œä¸æ–‡æ¡£ä¸­ â€œé€‚ç”¨èŒƒå›´ï¼šæ·˜å® & å¤©çŒ«å•†å®¶â€ çš„ä¸šåŠ¡é™å®šä¸€è‡´ğŸ”¶3-55ã€‚
# ä¸ ODS å±‚æµè½¬è¡”æ¥åˆç†
# æ‰€æœ‰ç»´åº¦è¡¨å‡åŸºäº ODS å±‚åŸå§‹æ•°æ®åŠ å·¥ç”Ÿæˆï¼Œä¿ç•™product_idâ€œshop_idç­‰æ ¸å¿ƒå…³è”å­—æ®µï¼Œä¸ºåç»­ DWD å±‚æ˜ç»†äº‹å®è¡¨çš„å…³è”åˆ†ææä¾›å¯é ç»´åº¦æ”¯æ’‘ï¼Œç¬¦åˆæ•°ä»“åˆ†å±‚æµè½¬é€»è¾‘ã€‚
#
# ä»£ç ç”Ÿæˆçš„ 5 å¼  DIM è¡¨å¯æœ‰æ•ˆæ”¯æ’‘å®¢æœä¸“å±ä¼˜æƒ ä¸šåŠ¡çš„å¤šç»´åº¦åˆ†æï¼Œå¦‚æŒ‰å•†å“ç±»å‹ã€å®¢æœã€å¹³å°ç­‰ç»´åº¦æ‹†è§£ä¼˜æƒ æ´»åŠ¨æ•ˆæœã€‚

