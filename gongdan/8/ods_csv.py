from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType
import os
import random
from datetime import datetime, timedelta
from decimal import Decimal

# å·¥å•ç¼–å·ï¼šå¤§æ•°æ® - ç”µå•†æ•°ä»“ - 08 - å•†å“ä¸»é¢˜è¥é”€å·¥å…·å®¢æœä¸“å±ä¼˜æƒ çœ‹æ¿


# é…ç½® PySpark ç¯å¢ƒ
os.environ["PYSPARK_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"
os.environ["PYSPARK_DRIVER_PYTHON"] = "D:\\ANACONDA\\envs\\day1\\python.exe"

# åˆå§‹åŒ–SparkSession
spark = SparkSession.builder \
    .appName("ç”Ÿæˆ700æ¡ODSå±‚æ•°æ®ï¼ˆæ¯è¡¨100æ¡ï¼‰") \
    .master("local[*]") \
    .getOrCreate()

# åˆ›å»ºå­˜å‚¨ç›®å½•
output_root = "mock_data/ods"
if not os.path.exists(output_root):
    os.makedirs(output_root)


# ç”Ÿæˆéšæœºæ—¥æœŸå·¥å…·å‡½æ•°
def random_date(start, end):
    return (start + timedelta(seconds=random.randint(0, int((end - start).total_seconds())))).strftime(
        "%Y-%m-%d %H:%M:%S")


# åŸºç¡€å‚æ•°è®¾ç½®ï¼ˆæ‰©å±•ä»¥æ”¯æŒæ¯è¡¨100æ¡æ•°æ®ï¼‰
start_date = datetime(2024, 12, 1)
end_date = datetime(2025, 3, 31)
shop_id = "shop_001"
activity_levels = ["å•†å“çº§", "SKUçº§"]
promo_types = ["å›ºå®šä¼˜æƒ ", "è‡ªå®šä¹‰ä¼˜æƒ "]
status_list = ["è¿›è¡Œä¸­", "å·²ç»“æŸ", "æœªå¼€å§‹"]
cs_ids = [f"cs_{i:03d}" for i in range(1, 21)]  # 20ä¸ªå®¢æœ
user_ids = [f"user_{i:03d}" for i in range(1, 301)]  # 300ä¸ªç”¨æˆ·
product_ids = [f"prod_{i:03d}" for i in range(1, 151)]  # 150ä¸ªå•†å“
sku_ids = [f"sku_{i:03d}" for i in range(1, 301)]  # 300ä¸ªSKU

# ------------------------------
# 1. æ´»åŠ¨è¡¨ï¼šods_customer_service_promo_activityï¼ˆ100æ¡ï¼‰
# ------------------------------
activity_schema = StructType([
    StructField("activity_id", StringType(), False),
    StructField("activity_name", StringType(), False),
    StructField("activity_level", StringType(), False),
    StructField("promo_type", StringType(), False),
    StructField("custom_promo_max", IntegerType(), True),
    StructField("start_time", StringType(), False),
    StructField("end_time", StringType(), False),
    StructField("status", StringType(), False),
    StructField("shop_id", StringType(), False),
    StructField("create_time", StringType(), False),
    StructField("raw_extra", StringType(), True)
])

activity_data = []
for i in range(1, 101):  # ç”Ÿæˆ100æ¡æ´»åŠ¨æ•°æ®
    act_id = f"act_{i:03d}"
    level = random.choice(activity_levels)
    p_type = random.choice(promo_types)
    custom_max = random.randint(5, 50) if p_type == "è‡ªå®šä¹‰ä¼˜æƒ " else None
    # æ´»åŠ¨æ—¶é•¿2-120å¤©
    start = random_date(start_date, end_date - timedelta(days=2))
    start_dt = datetime.strptime(start, "%Y-%m-%d %H:%M:%S")
    end_dt = start_dt + timedelta(days=random.randint(2, 120))
    end = end_dt.strftime("%Y-%m-%d %H:%M:%S")
    # çŠ¶æ€é€»è¾‘
    status = "å·²ç»“æŸ" if end_dt < datetime.now() else "è¿›è¡Œä¸­" if start_dt < datetime.now() else "æœªå¼€å§‹"
    # æ´»åŠ¨åç§°â‰¤10å­—
    names = ["åº—åº†ç‰¹æƒ ", "æ–°å®¢ç«‹å‡", "è€å®¢å›é¦ˆ", "é™æ—¶ä¼˜æƒ ", "èŠ‚æ—¥ä¿ƒé”€", "æ¸…ä»“æ´»åŠ¨", "ä¼šå‘˜ä¸“äº«",
             "æ¢å­£æŠ˜æ‰£", "æ–°å“ç‰¹æƒ ", "ç»„åˆä¼˜æƒ ", "æ»¡å‡æ´»åŠ¨", "ç§’æ€æ´»åŠ¨"]
    activity_data.append((
        act_id, random.choice(names), level, p_type, custom_max,
        start, end, status, shop_id,
        (start_dt - timedelta(days=random.randint(1, 7))).strftime("%Y-%m-%d %H:%M:%S"),
        f'{{"creator":"{random.choice(["è¿è¥éƒ¨", "å®¢æœéƒ¨"])}"}}'
    ))

activity_df = spark.createDataFrame(activity_data, activity_schema)
activity_df.write.mode("overwrite").option("header", "true").csv(f"{output_root}/ods_customer_service_promo_activity")

# ------------------------------
# 2. æ´»åŠ¨-å•†å“å…³è”è¡¨ï¼šods_customer_service_promo_activity_itemï¼ˆ100æ¡ï¼‰
# ------------------------------
item_schema = StructType([
    StructField("id", StringType(), False),
    StructField("activity_id", StringType(), False),
    StructField("product_id", StringType(), False),
    StructField("sku_id", StringType(), True),
    StructField("promo_amount", IntegerType(), False),
    StructField("limit_purchase_count", IntegerType(), False),
    StructField("is_removed", StringType(), False),
    StructField("create_time", StringType(), False)
])

item_data = []
for i in range(1, 101):  # ç”Ÿæˆ100æ¡å…³è”æ•°æ®
    item_id = f"item_{i:03d}"
    act_id = f"act_{random.randint(1, 100):03d}"  # å…³è”100ä¸ªæ´»åŠ¨
    prod_id = random.choice(product_ids[:100])  # ä»150ä¸ªå•†å“ä¸­é€‰100ä¸ª
    # å•†å“çº§æ´»åŠ¨æ— SKU
    act_level = [d[2] for d in activity_data if d[0] == act_id][0]
    sku_id = random.choice(sku_ids) if act_level == "SKUçº§" else None
    # ä¼˜æƒ é‡‘é¢â‰¤5000ä¸”ä¸ºæ•´æ•°
    promo_amt = random.randint(1, 200)
    # é™è´­æ¬¡æ•°é»˜è®¤1-5æ¬¡
    limit_cnt = random.randint(1, 5)
    is_removed = "æ˜¯" if random.random() < 0.1 else "å¦"  # 10%æ¦‚ç‡å·²ç§»å‡º
    create_t = random_date(start_date, datetime.now())
    item_data.append((item_id, act_id, prod_id, sku_id, promo_amt, limit_cnt, is_removed, create_t))

item_df = spark.createDataFrame(item_data, item_schema)
item_df.write.mode("overwrite").option("header", "true").csv(f"{output_root}/ods_customer_service_promo_activity_item")

# ------------------------------
# 3. ä¼˜æƒ å‘é€è¡¨ï¼šods_customer_service_promo_sendï¼ˆ100æ¡ï¼‰
# ------------------------------
send_schema = StructType([
    StructField("send_id", StringType(), False),
    StructField("activity_id", StringType(), False),
    StructField("product_id", StringType(), False),
    StructField("sku_id", StringType(), True),
    StructField("customer_service_id", StringType(), False),
    StructField("consumer_id", StringType(), False),
    StructField("actual_promo_amount", IntegerType(), False),
    StructField("valid_duration", IntegerType(), False),
    StructField("send_time", StringType(), False),
    StructField("remark", StringType(), True)
])

send_data = []
remarks = ["å’¨è¯¢æœªä¸‹å•", "ä»·æ ¼æ•æ„Ÿ", "æ–°å®¢å¼•å¯¼", "è€å®¢å¬å›", "æ´»åŠ¨æ¨å¹¿",
           "ä¸»åŠ¨å’¨è¯¢", "å”®åæ¨è", "å…³è”é”€å”®", "å¤è´­æ¿€åŠ±", "æµå¤±æŒ½å›"]
for i in range(1, 101):  # ç”Ÿæˆ100æ¡å‘é€æ•°æ®
    send_id = f"send_{i:03d}"
    act_id = f"act_{random.randint(1, 100):03d}"
    # å…³è”å•†å“ - ç¡®ä¿èƒ½æ‰¾åˆ°æœ‰æ•ˆçš„å…³è”é¡¹
    related_items = [d for d in item_data if d[1] == act_id and d[6] == "å¦"]
    if not related_items:  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„å…³è”é¡¹ï¼Œæ”¾å®½æ¡ä»¶
        related_items = [d for d in item_data if d[1] == act_id]
    item = random.choice(related_items) if related_items else random.choice(item_data)
    prod_id, sku_id = item[2], item[3]
    # å®é™…ä¼˜æƒ é‡‘é¢
    act_type = [d[3] for d in activity_data if d[0] == act_id][0]
    if act_type == "å›ºå®šä¼˜æƒ ":
        actual_amt = item[4]
    else:
        max_amt = [d[4] for d in activity_data if d[0] == act_id][0]
        actual_amt = random.randint(1, max_amt)
    # æœ‰æ•ˆæœŸ1-24å°æ—¶
    duration = random.randint(1, 24)
    # å‘é€æ—¶é—´åœ¨æ´»åŠ¨æœŸå†…
    act_start = [d[5] for d in activity_data if d[0] == act_id][0]
    act_end = [d[6] for d in activity_data if d[0] == act_id][0]
    send_t = random_date(datetime.strptime(act_start, "%Y-%m-%d %H:%M:%S"),
                         datetime.strptime(act_end, "%Y-%m-%d %H:%M:%S"))
    send_data.append((
        send_id, act_id, prod_id, sku_id, random.choice(cs_ids),
        random.choice(user_ids), actual_amt, duration, send_t,
        random.choice(remarks) if random.random() < 0.7 else None
    ))

send_df = spark.createDataFrame(send_data, send_schema)
send_df.write.mode("overwrite").option("header", "true").csv(f"{output_root}/ods_customer_service_promo_send")

# ------------------------------
# 4. ä¼˜æƒ æ ¸é”€è¡¨ï¼šods_customer_service_promo_useï¼ˆ100æ¡ï¼‰
# ------------------------------
use_schema = StructType([
    StructField("use_id", StringType(), False),
    StructField("send_id", StringType(), False),
    StructField("order_id", StringType(), False),
    StructField("pay_time", StringType(), False),
    StructField("pay_amount", DecimalType(10, 2), False),
    StructField("purchase_count", IntegerType(), False)
])

use_data = []
for i in range(1, 101):  # ç”Ÿæˆ100æ¡æ ¸é”€æ•°æ®
    use_id = f"use_{i:03d}"
    # ç¡®ä¿é€‰æ‹©çš„send_idæœ‰å¯¹åº”çš„æœ‰æ•ˆå…³è”é¡¹
    valid = False
    while not valid:
        send_id = f"send_{random.randint(1, 100):03d}"
        send_rec = [d for d in send_data if d[0] == send_id]
        if send_rec:
            send_rec = send_rec[0]
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„item_data
            matching_items = [d for d in item_data if d[2] == send_rec[2] and d[1] == send_rec[1]]
            if matching_items:
                valid = True

    # å…³è”å‘é€è®°å½•
    send_t = datetime.strptime(send_rec[8], "%Y-%m-%d %H:%M:%S")
    duration = send_rec[7]
    # æ ¸é”€æ—¶é—´åœ¨æœ‰æ•ˆæœŸå†…ï¼ˆæ”¯æŒè·¨å¤©ï¼‰
    pay_t = random_date(send_t, send_t + timedelta(hours=duration))
    # æ”¯ä»˜é‡‘é¢ï¼ˆè½¬ä¸ºDecimalç±»å‹ï¼‰
    pay_amt = Decimal(str(round(random.uniform(50, 1000), 2)))
    # è´­ä¹°æ•°é‡â‰¤é™è´­æ¬¡æ•°
    item_id = [d[0] for d in item_data if d[2] == send_rec[2] and d[1] == send_rec[1]][0]
    limit_cnt = [d[5] for d in item_data if d[0] == item_id][0]
    purchase_cnt = random.randint(1, limit_cnt)
    use_data.append((
        use_id, send_id, f"order_{i:03d}", pay_t, pay_amt, purchase_cnt
    ))

use_df = spark.createDataFrame(use_data, use_schema)
use_df.write.mode("overwrite").option("header", "true").csv(f"{output_root}/ods_customer_service_promo_use")

# ------------------------------
# 5. å•†å“è¡¨ï¼šods_productï¼ˆ100æ¡ï¼‰
# ------------------------------
product_schema = StructType([
    StructField("product_id", StringType(), False),
    StructField("product_name", StringType(), False),
    StructField("price", DecimalType(10, 2), False),
    StructField("shop_id", StringType(), False),
    StructField("status", StringType(), False)
])

product_data = []
prod_names = ["å«è¡£", "ç‰›ä»”è£¤", "è¿åŠ¨é‹", "Tæ¤", "å¤–å¥—", "è¿è¡£è£™", "èƒŒåŒ…", "æ‰‹è¡¨",
              "æ‰‹æœºå£³", "å……ç”µå™¨", "è€³æœº", "è£¤å­", "è¡¬è¡«", "æ¯›è¡£", "é‹å­", "å¸½å­"]
for pid in product_ids[:100]:  # ç”Ÿæˆ100æ¡å•†å“æ•°æ®
    # ä»·æ ¼è½¬æ¢ä¸ºDecimalç±»å‹
    price = Decimal(str(round(random.uniform(50, 1000), 2)))
    product_data.append((
        pid, f"{random.choice(prod_names)}{random.randint(101, 999)}",
        price, shop_id,
        "åœ¨å”®" if random.random() < 0.8 else "ä¸‹æ¶"
    ))

product_df = spark.createDataFrame(product_data, product_schema)
product_df.write.mode("overwrite").option("header", "true").csv(f"{output_root}/ods_product")

# ------------------------------
# 6. SKUè¡¨ï¼šods_skuï¼ˆ100æ¡ï¼‰
# ------------------------------
sku_schema = StructType([
    StructField("sku_id", StringType(), False),
    StructField("product_id", StringType(), False),
    StructField("spec", StringType(), False),
    StructField("sku_price", DecimalType(10, 2), False)
])

sku_data = []
specs = ["çº¢è‰²-S", "çº¢è‰²-M", "çº¢è‰²-L", "é»‘è‰²-S", "é»‘è‰²-M", "é»‘è‰²-L",
         "è“è‰²-S", "è“è‰²-M", "è“è‰²-L", "ç™½è‰²-S", "ç™½è‰²-M", "ç™½è‰²-L",
         "ç°è‰²-S", "ç°è‰²-M", "ç°è‰²-L"]
for sid in sku_ids[:100]:  # ç”Ÿæˆ100æ¡SKUæ•°æ®
    prod_id = random.choice(product_ids[:100])  # å…³è”100ä¸ªå•†å“
    # ä»å•†å“è¡¨æ•°æ®ä¸­è·å–åŸºç¡€ä»·æ ¼ï¼ˆå·²ä¸ºDecimalç±»å‹ï¼‰
    base_price = [d[2] for d in product_data if d[0] == prod_id][0]
    # è®¡ç®—SKUä»·æ ¼å¹¶ä¿æŒDecimalç±»å‹
    sku_price = Decimal(str(round(float(base_price) * (1 + random.uniform(-0.1, 0.2)), 2)))
    sku_data.append((sid, prod_id, random.choice(specs), sku_price))

sku_df = spark.createDataFrame(sku_data, sku_schema)
sku_df.write.mode("overwrite").option("header", "true").csv(f"{output_root}/ods_sku")

# ------------------------------
# 7. å®¢æœè¡¨ï¼šods_customer_serviceï¼ˆ100æ¡ï¼‰
# ------------------------------
cs_schema = StructType([
    StructField("customer_service_id", StringType(), False),
    StructField("cs_name", StringType(), False),
    StructField("shop_id", StringType(), False)
])

cs_data = []
cs_names = ["å¼ ä¸‰", "æå››", "ç‹äº”", "èµµå…­", "å­™ä¸ƒ", "å‘¨å…«", "å´ä¹", "éƒ‘å",
            "é’±ä¸€", "å­™äºŒ", "å‘¨å", "å´å››", "éƒ‘äº”", "ç‹å…­", "èµµä¸ƒ", "å­™å…«",
            "å‘¨ä¹", "å´å", "éƒ‘ä¸€", "é’±äºŒ"]
# æ‰©å±•å®¢æœIDä»¥æ”¯æŒ100æ¡æ•°æ®
cs_ids_extended = [f"cs_{i:03d}" for i in range(1, 101)]
for cid in cs_ids_extended:  # ç”Ÿæˆ100æ¡å®¢æœæ•°æ®
    cs_data.append((cid, random.choice(cs_names), shop_id))

cs_df = spark.createDataFrame(cs_data, cs_schema)
cs_df.write.mode("overwrite").option("header", "true").csv(f"{output_root}/ods_customer_service")

print(f"å·²ç”Ÿæˆ700æ¡æ•°æ®è‡³ {os.path.abspath(output_root)}ï¼ŒåŒ…å«7å¼ ODSè¡¨ï¼Œæ¯å¼ è¡¨100æ¡æ•°æ®")
spark.stop()



# è¡¨ç»“æ„è®¾è®¡ï¼šä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£ä¸­å®¢æœä¸“å±ä¼˜æƒ çš„ä¸šåŠ¡æµç¨‹ï¼Œè®¾è®¡äº† 7 å¼ æ ¸å¿ƒ ODS è¡¨ï¼ŒåŒ…æ‹¬æ´»åŠ¨è¡¨ã€æ´»åŠ¨ - å•†å“å…³è”è¡¨ã€ä¼˜æƒ å‘é€è¡¨ã€ä¼˜æƒ æ ¸é”€è¡¨ç­‰ï¼Œè¦†ç›–äº†ä»æ´»åŠ¨åˆ›å»ºåˆ°ä¼˜æƒ ä½¿ç”¨çš„å…¨é“¾è·¯æ•°æ®ğŸ”¶3-20ğŸ”¶3-23ğŸ”¶3-29ã€‚
# ä¸šåŠ¡è§„åˆ™é€‚é…ï¼š
# æ´»åŠ¨åç§°é™åˆ¶åœ¨ 10 å­—ä»¥å†…ï¼Œæ´»åŠ¨æ—¶é•¿è®¾ç½®ä¸º 2-120 å¤©ï¼Œç¬¦åˆæ–‡æ¡£ä¸­æ´»åŠ¨åˆ›å»ºçš„è§„åˆ™ğŸ”¶3-80ã€‚
# ä¼˜æƒ é‡‘é¢è®¾è®¡ä¸ºæ•´æ•°ä¸”ä¸è¶…è¿‡ 5000 å…ƒï¼Œè‡ªå®šä¹‰ä¼˜æƒ é‡‘é¢ä¸è¶…è¿‡è®¾ç½®çš„ä¸Šé™ï¼Œéµå¾ªäº†æ–‡æ¡£ä¸­å¯¹ä¼˜æƒ é‡‘é¢çš„é™åˆ¶ğŸ”¶3-99ğŸ”¶3-100ğŸ”¶3-88ã€‚
# ä¼˜æƒ æœ‰æ•ˆæœŸè®¾ç½®ä¸º 1-24 å°æ—¶ï¼Œæ”¯æŒè·¨å¤©æ ¸é”€åœºæ™¯ï¼Œä¸æ–‡æ¡£ä¸­ â€œæ”¯ä»˜æ¬¡æ•°å¯èƒ½å¤§äºå‘é€æ¬¡æ•°â€ çš„è¯´æ˜ä¸€è‡´ğŸ”¶3-41ğŸ”¶3-206ã€‚
# æ•°æ®å…³è”æ€§ï¼šé€šè¿‡activity_idå…³è”æ´»åŠ¨è¡¨ä¸æ´»åŠ¨ - å•†å“å…³è”è¡¨ï¼Œé€šè¿‡send_idå…³è”ä¼˜æƒ å‘é€è¡¨ä¸æ ¸é”€è¡¨ï¼Œç¡®ä¿äº†æ•°æ®é“¾è·¯çš„å®Œæ•´æ€§ï¼Œæ»¡è¶³åç»­åˆ†å±‚å¤„ç†æ—¶çš„å…³è”éœ€æ±‚ğŸ”¶3-37ğŸ”¶3-38ğŸ”¶3-40ã€‚
# å­˜å‚¨æ ¼å¼ï¼šæ‰€æœ‰è¡¨ä»¥ CSV æ ¼å¼å­˜å‚¨åœ¨mock_dataç›®å½•ä¸‹ï¼ŒåŒ…å«è¡¨å¤´ï¼Œä¿ç•™äº†åŸå§‹æ•°æ®æ ¼å¼ï¼ˆå¦‚æ—¶é—´å­—æ®µä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼‰ï¼Œç¬¦åˆ ODS å±‚ â€œåŸå§‹æ€§â€ çš„ç‰¹ç‚¹ğŸ”¶3-235ã€‚
#
# è¯¥ä»£ç ç”Ÿæˆçš„ 700 æ¡æ•°æ®ï¼ˆæ¯è¡¨ 100 æ¡ï¼‰å¯ä½œä¸ºæ•°ä»“ ODS å±‚çš„åŸºç¡€æ•°æ®ï¼Œä¸ºåç»­ DIMã€DWD ç­‰åˆ†å±‚çš„æ¸…æ´—è½¬æ¢æä¾›å¯é è¾“å…¥ã€‚

